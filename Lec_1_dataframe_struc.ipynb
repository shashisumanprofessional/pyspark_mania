{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "031affd3-f355-4059-b77e-ead5d1b00165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employee.csv  employee.json  employees.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls /data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f617ef-c95d-4c8c-84a7-ed2b174d1041",
   "metadata": {},
   "source": [
    "What is a DataFrame in PySpark?\n",
    "\n",
    "A DataFrame in PySpark is:\n",
    "\n",
    "A distributed collection of data\n",
    "\n",
    "Organized into rows and named columns\n",
    "\n",
    "Similar to:\n",
    "\n",
    "A table in a SQL database\n",
    "\n",
    "A Pandas DataFrame (but distributed across a cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47727a46-7dcd-4ce0-8645-c8150b5f4807",
   "metadata": {},
   "source": [
    "Key characteristics\n",
    "\n",
    "Immutable: You don‚Äôt change data in place; every transformation creates a new DataFrame\n",
    "\n",
    "Lazy evaluation: Operations are not executed immediately‚ÄîSpark builds a plan and runs it only when an action is called\n",
    "\n",
    "Optimized: Spark uses the Catalyst optimizer and Tungsten engine for efficient execution\n",
    "\n",
    "Schema-based: Each column has a defined data type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40388e7f-b058-49e4-b903-4b10ebed89f1",
   "metadata": {},
   "source": [
    "| id | name  | age |\n",
    "|----|-------|-----|\n",
    "| 1  | Alice | 24  |\n",
    "| 2  | Bob   | 30  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da417858-7a12-4182-957b-4cb0407c0fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/15 13:51:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark DataFrame Basics\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5884754a-27e1-42ab-a1ad-e72641da0422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|  Alice| 24|\n",
      "|  2|    Bob| 30|\n",
      "|  3|Charlie| 28|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, \"Alice\", 24),\n",
    "    (2, \"Bob\", 30),\n",
    "    (3, \"Charlie\", 28)\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"age\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc092b54-e1f1-4e15-adfe-0a342c0e01a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| _1|     _2| _3|\n",
      "+---+-------+---+\n",
      "|  1|  Alice| 24|\n",
      "|  2|    Bob| 30|\n",
      "|  3|Charlie| 28|\n",
      "+---+-------+---+\n",
      "\n",
      "root\n",
      " |-- _1: long (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"id\", \"name\", \"age\"]\n",
    "df = spark.createDataFrame(data)\n",
    "df.columns\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84a2d1a6-19ff-4e5b-a4ff-885379bda804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7013d25f-f3d5-484c-ad97-ea29acbf2e68",
   "metadata": {},
   "source": [
    "1Ô∏è‚É£ What is StructType?\n",
    "\n",
    "Think of StructType as a table blueprint ‚Äî it defines the schema of your DataFrame.\n",
    "\n",
    "It tells Spark:\n",
    "\n",
    "‚ÄúThis table has these columns‚Äù\n",
    "\n",
    "‚ÄúEach column has this type‚Äù\n",
    "\n",
    "üí° Analogy:\n",
    "StructType = the form template or Excel sheet header\n",
    "\n",
    "It doesn‚Äôt contain data, only structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e26f79-8819-4839-8d31-c3effe7217ff",
   "metadata": {},
   "source": [
    "What is StructField?\n",
    "\n",
    "StructField = one column definition inside the StructType.\n",
    "\n",
    "It defines:\n",
    "\n",
    "Column name\n",
    "\n",
    "Data type (StringType, IntegerType, etc.)\n",
    "\n",
    "Nullable or not (True/False)\n",
    "\n",
    "üí° Analogy:\n",
    "If StructType = Excel sheet,\n",
    "then each StructField = one column header with type info."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7e22da-50b4-4e50-9178-a765e9130cfc",
   "metadata": {},
   "source": [
    "| Concept     | Analogy                                                    |\n",
    "| ----------- | ---------------------------------------------------------- |\n",
    "| StructType  | The **table blueprint** (schema of entire DataFrame)       |\n",
    "| StructField | **One column** in the blueprint, with name, type, nullable |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d36b22-ec61-4711-b673-736b4d66efed",
   "metadata": {},
   "source": [
    "very common point of confusion üëç\n",
    "Short answer first, then details:\n",
    "\n",
    "‚ùó inferSchema, header, delimiter, etc. do NOT apply to this method\n",
    "(spark.createDataFrame(data, columns))\n",
    "\n",
    "They are only for file-based reads (CSV/JSON/etc.)\n",
    "\n",
    "What Spark Does Instead (Schema Inference Here)\n",
    "\n",
    "Spark infers schema automatically from Python types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b27cb8ff-2a35-47bc-a0c5-9c3b650c3282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  1|  Alice| 24|Engineering| 70000|\n",
      "|  2|    Bob| 30|  Marketing| 60000|\n",
      "|  3|Charlie| 28|      Sales| 55000|\n",
      "|  4|  David| 35|Engineering| 90000|\n",
      "|  5|    Eva| 26|         HR| 50000|\n",
      "|  6|  Frank| 40|    Finance| 95000|\n",
      "|  7|  Grace| 29|  Marketing| 62000|\n",
      "|  8|  Helen| 32|         HR| 58000|\n",
      "|  9|    Ian| 27|      Sales| 54000|\n",
      "| 10|   Jack| 45| Management|120000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv = spark.read.csv(\n",
    "    \"/data/employee.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "df_csv.show()\n",
    "df_csv.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afe9a99c-20f7-401a-85e8-4b95e8226f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---+-------+------+\n",
      "|age| department| id|   name|salary|\n",
      "+---+-----------+---+-------+------+\n",
      "| 24|Engineering|  1|  Alice| 70000|\n",
      "| 30|  Marketing|  2|    Bob| 60000|\n",
      "| 28|      Sales|  3|Charlie| 55000|\n",
      "| 35|Engineering|  4|  David| 90000|\n",
      "| 26|         HR|  5|    Eva| 50000|\n",
      "| 40|    Finance|  6|  Frank| 95000|\n",
      "| 29|  Marketing|  7|  Grace| 62000|\n",
      "| 32|         HR|  8|  Helen| 58000|\n",
      "| 27|      Sales|  9|    Ian| 54000|\n",
      "| 45| Management| 10|   Jack|120000|\n",
      "+---+-----------+---+-------+------+\n",
      "\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"/data/employee.json\")\n",
    "\n",
    "df.show()\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5140c604-abd6-4bc2-b45a-7f5bc68dbc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parque = [\n",
    "    (1, \"Alice\", 24, \"Engineering\", 70000),\n",
    "    (2, \"Bob\", 30, \"Marketing\", 60000),\n",
    "    (3, \"Charlie\", 28, \"Sales\", 55000),\n",
    "    (4, \"David\", 35, \"Engineering\", 90000),\n",
    "    (5, \"Eva\", 26, \"HR\", 50000),\n",
    "    (6, \"Frank\", 40, \"Finance\", 95000),\n",
    "    (7, \"Grace\", 29, \"Marketing\", 62000),\n",
    "    (8, \"Helen\", 32, \"HR\", 58000),\n",
    "    (9, \"Ian\", 27, \"Sales\", 54000),\n",
    "    (10, \"Jack\", 45, \"Management\", 120000)\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"age\", \"department\", \"salary\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89a14470-3910-498f-84bc-f905a89b881a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  1|  Alice| 24|Engineering| 70000|\n",
      "|  2|    Bob| 30|  Marketing| 60000|\n",
      "|  3|Charlie| 28|      Sales| 55000|\n",
      "|  4|  David| 35|Engineering| 90000|\n",
      "|  5|    Eva| 26|         HR| 50000|\n",
      "|  6|  Frank| 40|    Finance| 95000|\n",
      "|  7|  Grace| 29|  Marketing| 62000|\n",
      "|  8|  Helen| 32|         HR| 58000|\n",
      "|  9|    Ian| 27|      Sales| 54000|\n",
      "| 10|   Jack| 45| Management|120000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ParquetExample\").getOrCreate()\n",
    "\n",
    "df = spark.createDataFrame(data_parque, columns)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6c1a892-bc31-493a-88c1-8f4dd7446872",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/15 15:13:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "26/01/15 15:13:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "26/01/15 15:13:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "26/01/15 15:13:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "26/01/15 15:13:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "26/01/15 15:13:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "26/01/15 15:13:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.write.mode(\"overwrite\").parquet(\"/data/employees.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1594259-35a1-456c-acac-cd2a67d99877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  1|  Alice| 24|Engineering| 70000|\n",
      "|  4|  David| 35|Engineering| 90000|\n",
      "|  7|  Grace| 29|  Marketing| 62000|\n",
      "| 10|   Jack| 45| Management|120000|\n",
      "|  3|Charlie| 28|      Sales| 55000|\n",
      "|  6|  Frank| 40|    Finance| 95000|\n",
      "|  2|    Bob| 30|  Marketing| 60000|\n",
      "|  9|    Ian| 27|      Sales| 54000|\n",
      "|  8|  Helen| 32|         HR| 58000|\n",
      "|  5|    Eva| 26|         HR| 50000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_parquet = spark.read.parquet(\"/data/employees.parquet\")\n",
    "df_parquet.show()\n",
    "df_parquet.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82b0ee9b-ec0f-4d14-aed7-89fa2c2e391b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employee.csv  employee.json  employees.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls /data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec58c1d-5c4b-4975-b108-4b9910a9fed5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
